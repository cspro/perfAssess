<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<title>Short Constructed-Response &amp; Technology-Enhanced Items</title>
	</head>

	<body>
		<nav class="floatingMenu">
			<a href="#approach" class="selected">Approach</a>
			<a href="#exemplars">Exemplars</a>
			<a href="#moreinfo">More Info</a>
		</nav>
		<div class="approachPage">
			<section id="approach">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>Short Constructed-Response<br/> 
							&amp; Technology-Enhanced Items</h1>
					</article>
				</div>
				<hr />
				<article>
					<div class='brainsharkWrapper'></div>
					<h2>Short Constructed-Response</h2>
					<p> Short constructed-response (SCR) items, whether paper-based or online-administered, require examinees to develop a partial or full response to a stimulus or prompt as opposed to selecting from a limited set of pre-specified options. </p>
					<h5>In general, features of SCR items include:</h5>
					<ul>
						<li> Elicit textual or graphical responses that are discrete and brief </li>
						<li> Often allow multiple appropriate answers and/or partial credit scoring </li>
						<li> May better support diagnosis and remediation of certain specific skill deficiencies than selected-response items </li>
						<li> May be human or machine scorable </li>
					</ul>
					<h2>Technology-Enhanced Items </h2>
					<p> Technology-enhanced items (TEIs) feature technology enhancements— such as the addition of interactive functionality, sound, graphics, animation, or video—intended to substantially improve some aspect of measurement. </p>
					<h5>In general, features of TEIs include:</h5>
					<ul>
						<li> Elicit responses that are discrete and brief </li>
						<li> Are typically machine scorable </li>
						<li> Expand the range of content, knowledge, or skills that are assessable in large-scale contexts compared to traditional paper-and-pencil formats </li>
					</ul>
				</article>
			</section>
			<section id="exemplars">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>Examples of Short Constructed-Response<br/>
							&amp; Technology-Enhanced Items</h1>
					</article>
				</div>
				<hr />
				<article>
					<h3>Below is a list of examples that demonstrate some of the ways that short constructed-response and technology-enhanced items may be used as performance assessment.</h3>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://sampleitems.smarterbalanced.org/itempreview/sbac/ELA.htm">Smarter Balanced Assessment Consortium - Sample ELA TEI Items</a></h3>
					<p>The Smarter Balanced Assessment Consortium (SBAC) has published English Language Arts (ELA) technology-enhanced items (TEI) prototypes that demonstrate a variety of technical tools that can be used to measure student knowledge, skills, and abilities. To view these, use the filter tool on the SBAC site to identify the items they have categorized TEIs.</p>
					<p>SBAC. Retrieved February 27, 2013 from <a href="http://www.smarterbalanced.org/">http://www.smarterbalanced.org/ </a></p>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="https://test.testnav.com/refqc/testnav-7.5.12.70/epatLogin.jsp?testnavTestId=cssmini&testnavFormId=cssmini">DCD Developed TEIs and Short Constructed-Response Items</a></h3>
					<p>Pearson's Digital Content Development team has developed a number of highly innovative tools and features that are used for technology-enhanced and other item types delivered online within the TestNav™ online delivery system. This link provides two examples: one using a graphing tool and providing space for a constructed response and another that allows students to create a presentation within an online form. </p>
					<p>Note the sample items will be launched in TestNav using a non-secure practice tool. To run TestNav, your machine must meet the minimum requirements found here:<a href="http://www.pearsononlinetesting.com/TestNav/7/requirements_testnav_7_5_10.html"> http://www.pearsononlinetesting.com/TestNav/7/requirements_testnav_7_5_10.html</a></p>
				</article>
			</section>
			<section id="moreinfo">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>What Are Short Constructed-Response<br />
							&amp; Technology-Enhanced Items?</h1>
					</article>
				</div>
				<hr />
				<dl class="accordion">
					
					<dt><a href="">Definition</a></dt>
						<dd>
							<p><strong>Short constructed-response (SCR) items</strong>, whether paper-based or online-administered, require examinees to develop a partial or full response (textual or graphical) to a stimulus or item stem as opposed to selecting from a limited set of pre-specified options. These items elicit discrete and brief responses, in contrast to more extended constructed responses elicited with other assessment approaches, such as essay prompts. </p>
							<p>SCRs often support multiple appropriate answers, both with and without partial credit scoring. As a result of these features, SCR items enable more direct measurement of certain types of knowledge and skills than selected-response (SR) items. Because these items are less constrained than SR items, they often appear more similar to the types of open-ended problems typically encountered in academic and work settings (i.e., they offer more realistic or authentic problem representations). SCRs may provide more detailed information about specific error tendencies than SR items; as such, they may better support diagnosis and remediation of specific skill deficiencies. Finally, certain types of SCRs (e.g., computer-administered items that call for a graphical response) are machine scorable, which can help to partially offset the higher development costs associated with these items.</p>
							<p><strong>Technology-enhanced items (TEIs)</strong> feature technology enhancements—such as the addition of interactive functionality, sound, graphics, animation, or video—to the stimulus materials, item stem, response options, and/or response mode, where the technology is intended to substantially improve some aspect of measurement. These items elicit discrete and brief responses compared to more continuous digital assessment approaches, such as online games and simulated environments. </p>
							<p>TEIs are typically machine scorable, supporting either dichotomous or partial credit scoring models. As a result of these features, TEIs may expand the range of skills that are practically assessable in large-scale contexts compared to traditional paper-and-pencil formats. By virtue of their enhanced interactivity and functionality, TEIs may be more appealing and engaging to examinees than static, paper-based items. Finally, some types of TEIs may better support formative use relative to traditional paper-and-pencil items because they can be scored automatically and provide real-time performance feedback.</p>
							<p>There is a substantial amount of overlap between SCR and TEI types: many SCR items, if administered online with additional features or functionality that enhance measurement quality in relation to what can be achieved via paper, would also be considered TEIs. However, SCR items are not necessarily technology-enhanced: SCRs are commonly administered via paper-and-pencil in the context of classroom or formative assessment. Similarly, not all TEIs call for constructed responses: some computer-administered selected-response items make use of technological enhancements to better measure complex reasoning or other higher-order thinking skills. Finally, not all TEIs and SCRs can be considered examples of performance assessment. For example, items that don&rsquo;t meet the criterion of assessing complex reasoning or other higher-order thinking skills (a short-answer item or online version of a SR item that focuses on simple recall) would not be included within the framework.</p>
						</dd>
					
					<dt><a href="">Characteristic Features</a></dt>
						<dd>
							<h5>Short Constructed-Response Items</h5>
							<ul>
								<li>Whether in a paper-based or computer-administered environment, focus on production, development, or composition as opposed to selection from among pre-specified options </li>
								<li>Elicit textual or graphical responses that are discrete and brief </li>
								<li>Often allow multiple appropriate answers and/or partial credit scoring </li>
								<li>Enable more direct measurement of certain types of skills than selected-response (SR) formats (e.g., graphing, editing) </li>
								<li>May offer more realistic or authentic problem representations than SR items </li>
								<li>May better support diagnosis and remediation of certain specific skill deficiencies than SR items </li>
								<li>May be human or machine-scorable </li>
							</ul>
							<h5>Technology-Enhanced Items</h5>
							<ul>
								<li>Offer technological enhancements—such as the addition of interactive functionality, sound, graphics, animation, or video—to the stimulus materials, item stem, response options, and/or response mode, where the technology substantially improves some aspect of measurement by measuring something better (more validly, reliably, or efficiently) or measuring more (increased range of content, knowledge, or skills)</li>
								<li>Elicit responses that are discrete and brief compared to more continuous digital approaches, such as online games and simulated environments </li>
								<li>Are typically machine scorable (either dichotomously or polytomously) </li>
								<li>Expand the range of content, knowledge, or skills that are assessable in large-scale contexts compared to traditional paper-and-pencil formats, by either supporting new methods for capturing diverse response modes (e.g., to include speaking, listening, or presentation skills) or by making certain types of performances machine scorable that would have been hand-scored if administered via paper (e.g., creating a number line, graphing) </li>
								<li>May be more appealing and engaging to examinees than static, paper-based items by virtue of their enhanced interactivity and functionality </li>
								<li>May offer greater potential for formative assessment relative to traditional paper-and-pencil items vis-à-vis automated scoring and provision of real-time feedback </li>
							</ul>
						</dd>
					
					<dt><a href="">Design Variations and Other Considerations</a></dt>
						<dd>
							<h5>Short Constructed-Response Items</h5>
							<p>SCR items can come in many different forms, depending on the type of response collected from the examinee—whether textual or graphical—including short-answer items, figural constructed response items, concept maps (Chung &amp; Baker, 1997; Yin et al., 2005), and editing exercises (Breland, 1999; Davey, Godwin, &amp; Mittelholtz, 1997), among others. Short-answer items might require examinees to compose a few words or a few sentences in response to a specific question, to solve a mathematical problem and provide a short explanation of the solution, or to identify elements in a series or list. Figural constructed-response items require examinees to complete, correct, or create drawings, illustrations, or graphics, such as graphs and charts. Concept maps require examinees to complete or construct a graphic representation of some topic by organizing and illustrating relationships between key terms. Editing exercises present examinees with a flawed writing sample and require them to make revisions to improve the writing.</p>
							<p>There are other examples of SCR item formats, and within the formats described above, there are additional variations depending on the degree of constraint or open-endedness of the item. What all of these item formats have in common is that examinees are required to produce, develop, or construct their responses rather than select from among a limited set of pre-specified options. However, the level of production or development can vary across different item types, ranging from partial development (e.g., sentence completion or equation correction) to full development (construction of a total unit or response, as in a short answer item).</p>
							<h5>Technology-Enhanced Items</h5>
							<p>TEIs can also come in many different forms, depending on the response demand and response mode. Scalise and Gifford (2006) identified a continuum of TEI types, ranging from most to least constrained. Many of the item types in their continuum overlap with commonly-mentioned SCR item types, such as figural constructed response items and concept maps. However, TEIs may also include more constrained item types than are typically associated with SCR items, such as reordering or rearrangement items, where responses are selected from a pre-specified set, but examinees are required to order or sequence their responses in some way. As noted by Parshall et al. (2002), one of the benefits of TEIs compared to paper-based items is that TEIs can reduce the impact of random guessing by substantially increasing the number of possible response options available in more constrained TEI types.</p>
							<p>What all of these item types have in common is that the item features some form of technology enhancement—in the form of embedded media or additional functionality—that enables better, more direct, or more complete measurement of the targeted skill. However, as with SCRs, the level of production or development called for can vary across different item types, ranging from partial development (e.g., editing a writing sample) to full development (construction of a total unit or response, as in graph construction). </p>
							<p>TEIs can also be distinguished along other dimensions. For example, items may or may not incorporate some form of media, such as sound, animation, or video; and items can differ with respect to the types of associated functionalities they incorporate, such as the ability to rotate, resize, zoom, pause, or playback. Items can also differ with respect to the level of interactivity supported, or the extent to which the item responds or reacts to examinee inputs. Interactivity can range from &ldquo;passive&rdquo; interaction, in which the interface responds to examinee inputs by displaying in real-time the results of any changes or actions applied (e.g., a bar chart that allows examinees to change the height of bars by dragging them), to more interactive applications (e.g., multi-step, adaptive items or items that can provide immediate feedback based on the examinee&rsquo;s response). Parshall et al. (2000) point out that, although increased interactivity improves the &ldquo;realism&rdquo; of the examinee&rsquo;s experience compared to more static item types, it also increases the complexity of scoring.</p>
						</dd>
					
					<dt><a href="">Response Demands</a></dt>
						<dd>
							<h5>Short Constructed-Response Items</h5>
							<p>The response demands of SCR items can vary depending on the level of constraint of the particular SCR format used. As previously explained, SCR items require examinees to develop, produce, or create a partial or full response to an item (as opposed to selecting from among a limited set of pre-specified response options). However, that response can take many forms, ranging from a few words or sentences to a graphic representation of a topic that illustrates the organization of and relationships among key concepts. Depending on the level of constraint of the item type used, SCRs may require examinees to complete or correct a set of stimulus materials (partial response) or to formulate a total idea unit (full response). Due to relaxed levels of constraint compared to traditional selected-response (SR) item types, SCRs are said to allow better or more direct measurement of hard-to-assess knowledge and skills, such as complex reasoning and other higher-order thinking skills (Bennett, 1993; Bennett et al., 1991). </p>
							<h5>Technology-Enhanced Items</h5>
							<p>Similar to SCRs, TEIs can demand a wide range of responses from examinees depending on the level of constraint of the particular TEI format used. As with SCRs, examinees may be required to develop, produce, or create a partial or full response to an item. In addition, although all TEIs require examinees to interact with a computer interface in some way, the range of possible response actions, or the means by which examinees can provide their responses (e.g., keyboard entry, mouse clicks, touch screens, or speech recognition software) is broad. Due to relaxed levels of constraint, as well as enhanced examinee interest and engagement compared to more static items, TEIs are said to allow better or more direct measurement of hard-to-assess knowledge and skills, such as complex reasoning and higher-order thinking skills (Parshall et al., 2000; Scalise &amp; Gifford, 2006). In addition, the increased range of response actions may reduce the impact of construct-irrelevance associated with limited response modes by making items more accessible to all students (Dolan et al., 2011).</p>
						</dd>
					
					<dt><a href="">Evaluation Criteria and Procedures</a></dt>
						<dd>
							<h5>Short Constructed-Response Items</h5>
							<p>SCR items are typically scored using rubrics that identify one or more appropriate or correct responses. Rubrics may also identify responses characterized by varying degrees of quality or correctness, thereby supporting partial credit scoring models. Scoring complexity varies depending on whether items are scored dichotomously or polytomously, as well as the degree of constraint of the item type. For example, open-ended items that allow partial credit scoring and require full or complete construction imply greater scoring complexity than more constrained items that require only partial construction and are scored dichotomously. Currently, some types of SCR items are machine-scorable using either rule-based scoring methods or algorithms based on natural language processing methods (Leacock, 2004).</p>
							<h5>Technology-Enhanced Items</h5>
							<p>Scoring method also varies across different TEI types. Less constrained TEI types typically require a scoring rubric identifying one or more appropriate or correct responses. In addition, as with SCR items, some TEIs are scored dichotomously, whereas others are scored polytomously, supporting partial credit scoring models. The degree of interactivity can also impact scoring complexity, both at the micro and macro assessment levels. For example, more interactive TEIs require more complex scoring rules, and assessments composed of greater numbers of TEIs require more complex scoring models that can handle large amounts of scoring dependency both across and within items (Parshall et al., 2000). TEIs are typically machine-scored using either rule-based scoring methods or algorithms based on natural language processing methods (Scalise &amp; Gifford, 2006). </p>
						</dd>
					
					<dt><a href="">Administration Time</a></dt>
						<dd>
							<p>Short, brief; response times can range from one to several minutes</p>
						</dd>
					
					<dt><a href="">References</a></dt>
						<dd>
							<p>Bennett, R. E. (1993). On the meanings of constructed response. In R. E. Bennett &amp; W. C. Ward (Eds.), <em>Construction versus choice in cognitive measurement: Issues in constructed response, performance testing, and portfolio assessment </em>(pp. 1-27). Hillsdale, NJ: Lawrence Erlbaum Associates. </p>
							<p>Bennett, R. E., Sebrechts, M. M., &amp; Yamamoto, K. (1991). <em>Fitting new measurement models to GRE General Test constructed-response item data </em>(RR-91-60). Princeton, NJ: Educational Testing Service. </p>
							<p>Breland, H. M. (1999). <em>Exploration of an automated editing task as a GRE Writing measure</em>. (RR-99-9). Princeton, NJ: Educational Testing Service. </p>
							<p>Chung, G. &amp; Baker, E. (1997<em>). Year 1 technology studies: Implications for technology in assessment</em> (CSE Technical Report No. 459). Los Angeles: Center for the Study of Evaluation, National Center for Research on Evaluation, Standards, and Student Testing, University of California, Los Angeles. </p>
							<p>Davey, T., Godwin, J., &amp; Mittelholz, D. (1997). Developing and scoring an innovative computerized writing assessment. <em>Journal of Educational Measurement</em>, <em>34</em>, 21–41. </p>
							<p>Dolan, R. P., Goodman, J., Strain-Seymour, E., Adams, J., &amp; Sethuraman, S. (2011).<em>Cognitive lab evaluation of innovative items in mathematics and English/ language arts assessment of elementary, middle, and high school students</em>. Research Report. Iowa City: Pearson. </p>
							<p>Leacock, C. (2004). Scoring free-responses automatically: A case study of a large-scale assessment. <em>Examens</em>, 1 (3). </p>
							<p>Osterlind, S. J. (1998). <em>Constructing test items: Multiple-choice, constructed-response, performance, and other formats</em>. Norwell, MA: Kluwer Academic Publisher. </p>
							<p>Parshall, C. G., Davey, T., &amp; Pashley, P. J. (2000). Innovative item types for computerized testing. In W. Van der Linden, Glas, C. A. W. (Ed.),<em>Computerized adaptive testing: Theory and practice</em> (pp. 129–148). Norwell, MA: Kluwer Academic Publisher. </p>
							<p>Parshall, C. G., Spray, J., Kalohn, J., &amp; Davey, T. (2002). Issues in innovative item types. In <em>Practical considerations in computer-based testing</em> (pp. 70–91). New York: Springer. </p>
							<p>Scalise, K., &amp; Gifford, B. (2006). Computer-based assessment in e-learning: A framework for constructing &ldquo;intermediate constraint&rdquo; questions and tasks for technology platforms. <em>Journal of Technology, Learning, and Assessment</em>,<em>4</em>(6). </p>
							<p>Yin, Y., Vanides, J., Ruiz-Primo, M. A., Ayala, C. C., &amp; Shavelson, R. J.  (2005). Comparison of two concept-mapping techniques: Implications for scoring, interpretation, and use. <em>Journal of Research in Science Teaching</em>, <em>42</em>(2): 166-184.</p>
						</dd>
						
				</dl>
			</section>
		</div>
	</body>
</html>
