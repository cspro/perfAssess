<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<title>Essays</title>
	</head>

	<body>
		<nav class="floatingMenu">
			<a href="#approach" class="selected">Approach</a>
			<a href="#exemplars">Exemplars</a>
			<a href="#moreinfo">More Info</a>
		</nav>
		<div class="approachPage">
			<section id="approach">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>Essays</h1>
					</article>
				</div>
				<hr />
				<article>
					<div class='brainsharkWrapper'></div>
					<h2>Essays</h2>
					<p>Essays require examinees to compose an extended piece of writing in response to a specific prompt.</p>
					<h5>In general, features of essays include:</h5>
					<ul>
						<li>Requiring students to compose an extended piece of writing (in contrast to short constructed-response items)</li>
						<li>Posing a specific writing prompt that often identifies the purpose for writing, the intended audience, and desired structural or content-related features of the response</li>
						<li>Providing a more direct measure of writing skills than selected-response formats</li>
						<li>Scoring using one or more rubrics, which may be analytic or holistic</li>
						<li>Human or machine scoring </li>
					</ul>
					
					<div class="pageButtons">
						<a class="pageButton" href="#_exemplars">Exemplars &raquo;</a><a class="pageButton" href="#_moreinfo">More Info &raquo;</a>
					</div>
					
				</article>
			</section>
			<section id="exemplars">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>Examples of Essays</h1>
					</article>
				</div>
				<hr />
				<article>
					<h3>Below is a list of examples that demonstrate some of the ways that essays may be used as performance assessment.</h3>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://nces.ed.gov/nationsreportcard/">National Assessment of Educational Progress</a></h3>
					<p>The National Assessment of Educational Progress (NAEP) has released several writing prompts and rubrics from previous administrations.</p>
					<p>Released writing prompts (grades 4, 8, and 12): <a href="http://nces.ed.gov/nationsreportcard/itmrlsx/search.aspx?subject=writing">http://nces.ed.gov/nationsreportcard/itmrlsx/search.aspx?subject=writing</a></p>
					<p>Rubrics: <a href="http://www.nagb.org/content/nagb/assets/documents/publications/frameworks/writing-2011.pdf">http://www.nagb.org/content/nagb/assets/documents/publications/frameworks/writin g-2011.pdf</a></p>
					<p><em>NAEP. Retrieved March 14, 2013 from</em> <a href="http://nces.ed.gov/nationsreportcard/">http://nces.ed.gov/nationsreportcard/ </a></p>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://ok.gov/sde/accountability-assessments">Oklahoma State Department of Education, Oklahoma Core Curriculum Tests</a></h3>
					<p>The Oklahoma State Department of Education (ODE) includes writing in their Oklahoma Core Curriculum Tests (OCCT) for students in grade 5. As an example of what might appear on the assessment, the ODE has provided the public with a narrative writing prompt and rubric.</p>
					<p>Grade 5 narrative writing prompt: <a href="http://ok.gov/sde/sites/ok.gov.sde/files/Gr5_Narrative_Prompt.pdf">http://ok.gov/sde/sites/ok.gov.sde/files/Gr5_Narrative_Prompt.pdf</a></p>
					<p>Rubric: <a href="http://ok.gov/sde/sites/ok.gov.sde/files/CCSS_Gr5_Trans_Rubric.pdf">http://ok.gov/sde/sites/ok.gov.sde/files/CCSS_Gr5_Trans_Rubric.pdf</a></p>
					<p><em>ODE. Retrieved March 14, 2013 from</em> <a href="http://ok.gov/sde/">http://ode.gov/sde</a></p>
				</article>
				
				<hr class="subDivider" />
				<article>
					<div class="pageButtons">
						<a class="pageButton" href="#_approach">Approach &raquo;</a><a class="pageButton" href="#_moreinfo">More Info &raquo;</a>
					</div>
				</article>
				
			</section>
			<section id="moreinfo">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>What Are Essays?</h1>
					</article>
				</div>
				<hr />
				<dl class="accordion">

					<dt><a href="">Definition</a></dt>
					<dd>
						<p>Essay prompts require students to compose an extended piece of writing in response to a specific writing prompt. Such responses are lengthier than <a href="#scrtei">short constructed responses</a>, capable of treating a given topic or purpose in more depth and providing students more opportunities to display composition skills. Essay prompts often identify the purpose for writing, the intended audience, and/or one or more desired structural or content-related features of the response. By virtue of their emphasis on composition, production, or development (as opposed to selection among alternatives), essays provide a more direct measure of a wider range of writing skills and processes compared to more constrained assessment approaches (selected-response items focusing on editing subskills, for example). </p>
						<p>In general, when used in large-scale summative assessment contexts (e.g., national or state writing assessment programs), essays constitute discrete writing samples, under timed and relatively constrained administration conditions, with typically no opportunities for examinees to create successive drafts, receive feedback, and make revisions. Although these conditions are contrary to good writing practices, strict standardization of assessment conditions is important for maintaining comparability and score quality in summative contexts. When used to support formative assessment, on the other hand, essays can more closely model the stages of an authentic writing process (e.g., including prewriting/planning, drafting, peer reviewing, and revising) and can be distributed over multiple occasions. Essays are either human or machine-scored using one or more rubrics or sets of scoring criteria that recognize varying gradations of response quality, as well as attributes or features that characterize each level. Rubrics may require an overall judgment of response quality (holistic) or require separate judgments of response quality along multiple dimensions or traits (analytic).</p>
					</dd>

					<dt><a href="">Characteristic Features</a></dt>
					<dd>
						<p>Essays can support a variety of writing purposes. Examples of different types of prompts for different purposes include:</p>
						<ul>
							<li>Explaining or analyzing via reading to write prompts (e.g., &ldquo;Summarize the main idea presented in the text…&rdquo;)</li>
							<li>Telling a story through a personal narrative (e.g., &ldquo;Tell a true story about a time you overcame a challenge…&rdquo;)</li>
							<li>Persuading others through argument-based prompts (e.g., &ldquo;Use data and evidence from the text to evaluate the claim that…&rdquo;)</li>
							<li>Representing a scene or event using sensory details through descriptive prompts (e.g., &ldquo;describe your favorite place so that someone reading your essay could imagine it&rdquo;) </li>
						</ul>
					</dd>

					<dt><a href="">Design Variations and Other Considerations</a></dt>
					<dd>
						<p>Essay prompts can vary along a number of dimensions. First, prompts may focus on assessing writing skills, mastery of subject-matter or content knowledge, or both. If both writing skills and content knowledge are assessed, distinct rubrics may be developed to produce separate scores—one for the content being assessed and one for the quality of the writing.</p>
						<p>Prompts may also vary with respect to whether they only require students to produce writing (writing only prompts) or whether they also require students to read or otherwise synthesize and integrate a set of stimulus materials (e.g., passages, charts, graphs, or images) into their written responses (reading to write prompts).</p>
						<p>Some assessment programs allow students to choose from among several topics, whereas others prescribe the use of a specific prompt for all students in order to ensure comparability of scores. Research on the impact of student choice on performance has been mixed, with some evidence suggesting that giving students choices allows them to demonstrate their maximal performance but does not alter the meaning of scores (Bridgeman, Morgan, &amp; Wang, 1997), and other evidence suggesting that examinees are not equally skilled in selecting the &ldquo;best&rdquo; prompt, resulting in scores that are not comparable (Linn, Betebenner, &amp; Wheeler, 1998; Wainer &amp; Thissen, 1994).</p>
						<p>Essay prompts also vary by discourse mode (Huot, 1990). Narrative writing prompts, which can be personal or imaginative, require students to recount an event or tell a story; descriptive writing prompts ask examinees to use sensory details to depict a person, place, or thing; expository writing provides information about or an explanation of an issue or topic; and argument-based essays ask writers to use evidence or reason to evaluate the veracity of a claim.</p>
						<p>Prompts may also differ with respect to the level of rhetorical specification provided (Huot, 1990). Prompts may range from relatively open-ended (identifying only the topic, for example) to highly specified (identifying topic, purpose for writing, intended audience, and context). More open-ended prompts may be more difficult to score reliably than highly specified prompts, due to greater variability in how students approach the task. In addition, some writing proponents claim that highly specified prompts are more authentic to the types of writing students will perform in future settings (e.g., in professional contexts). However, in high-stakes writing assessments, even open-ended prompts implicitly specify both a purpose (obtain the highest score possible) and an audience (trained rater). Moreover, the addition of lengthy contexts may not improve writing performance and may introduce construct-irrelevant variance through unnecessary reading load.</p>
						<p>Prompts may also vary with respect to the amount of structure provided to support student responses. For example, some prompts may simply specify what the final writing product should look like, whereas others may direct students to brainstorm ideas, draft, and revise. Some essay prompts may even provide material supports for more extended writing processes, including graphic organizers to help students prepare and plan their responses.</p>
						<p>Variations in wording of essay prompts are also possible (e.g, personal versus neutral, question versus command). Research evidence is mixed as to whether minor variations in prompt wording have an effect on student performance (Huot, 1990). Essays may be delivered on paper or computer-administered. Research generally suggests comparability of scores from online and paper-based administrations of essays, assuming any handwritten responses are typed or transformed using a word processing program prior to scoring (Bennett, 2003).</p>
						<p>Finally, essay prompts can support both formative and summative uses. In formative settings, the essay approach may more closely resemble an authentic writing process, with writing distributed over several occasions and opportunities for students to plan, create successive drafts, receive feedback, and revise their writing. In most large-scale summative settings, where strict standardization is more important to ensure comparability, writing may be limited to a single occasion, students typically do not receive interim feedback on drafts, and the prompts themselves (e.g., topics, discourse mode, level of specification, wording) are often more carefully controlled. There are exceptions to this rule, however. A few state or district writing assessment programs provide lengthier testing windows, multiple writing occasions, and opportunities for students to draft and revise. However, none of these programs provides the option for students to receive teacher or peer feedback on their drafts.</p>
					</dd>

					<dt><a href="">Response Demands</a></dt>
					<dd>
						<p>All essays require students to compose a piece of writing on demand. Even when pre-writing planning activities are not explicitly required or scored, planning may be necessary in order to complete a response that addresses the prompt within the time limits. However, planning behavior probably varies as a function of writing ability, with stronger writers presumably engaging in more planning than weak writers.</p>
						<p>Response demands may vary depending on the type of prompt (whether writing only or reading to write). With the former type of prompt, students are only required to read the prompt itself and their responses are scored only on the basis of their writing skills. With the latter type, students may also have to read, comprehend, and integrate information from associated stimulus materials, such as passages, graphs, or charts. Further, their responses may be evaluated with respect to mastery of specific subject-matter content in addition to quality of the writing. Different discourse modes may also place different response demands on students (Huot, 1990; Quellmalz et al., 1980). Students creating an imaginative narrative, for example, will probably use different writing strategies than students creating an argument. Research demonstrates that student performance tends to vary across different discourse modes, suggesting the need to sample from different types of writing in order to make valid inferences about general writing ability (Huot, 1990; Quellmalz et al., 1980).</p>
						<p>Finally, response demands may differ by examinee subgroup. Research shows persistent subgroup differences in essay performance by gender (Willingham et al., 1997), ethnicity (Applebee &amp; Langer, 2006), and native language status (Heck &amp; Crislip, 2001), with the most recent NAEP results exhibiting all three types of differences. These disparities in performance may indicate real differences in writing ability rather than bias. However, they may also indicate differences in the writing strategies used by different types of examinees. More research is needed in this area.</p>
					</dd>

					<dt><a href="">Evaluation Criteria and Procedures</a></dt>
					<dd>
						<p>Essay responses are generally scored using one or more rubrics. Holistic rubrics provide an overall judgment of response quality, whereas analytic rubrics provide separate judgments of response quality along multiple dimensions or traits. Although the writing community is not in complete agreement on the qualities of &ldquo;good&rdquo; writing, commonly assessed traits include content, organization, development, voice, vocabulary, language, mechanics, and coherence. </p>
						<p>There is a large body of research on the relative advantages and disadvantages of analytic and holistic rubrics. This body of research suggests some general conclusions: holistic scoring is faster and less expensive than analytic scoring, but analytic scores may be more reliable; scores assigned using analytic and holistic rubrics tend to be very highly correlated; only a few writing traits can be reliably distinguished using an analytic rubric; and analytic rubrics better support instruction and learning than holistic rubrics because they provide more information regarding specific strengths and weaknesses (Huot, 1990; Quellmalz, 1984). However, research also suggests that when multiple analytic scores are assigned by the same rater (which is a common scoring method due to cost efficiency), there is the potential for halo error. In other words, raters&rsquo; perceptions of one trait may affect their perceptions of other traits, leading to spuriously high correlations between separate traits (Lai, Wolfe, &amp; Vickers, 2012).</p>
						<p>Essays may be human- or machine-scored. Research on automated scoring systems demonstrates high agreement between automated scores and scores assigned by human raters (Attali, 2007; Landauer, Laham, &amp; Foltz, 2003; Nichols, 2004; Page, 2003). There are currently a number of different automated scoring approaches that differ from one another primarily with respect to their underlying scoring algorithms. However, they all attempt to predict human scores by tagging response features that characterize different levels of quality and aggregating those features into some sort of meaningful composite. Different scoring models recognize different response features and may aggregate those features differently. Scoring engines are trained and calibrated with human-scored responses. Therefore, the quality of the human-scored calibration set (e.g., representativeness, accuracy and consistency of scores) is a crucial contributor to the quality of automated scores.</p>
					</dd>

					<dt><a href="">Administration Time</a></dt>
					<dd>
						<p>Most large-scale essay prompts range from 25 to 90 minutes. In contrast, essays administered in the classroom to support formative use might be distributed over several days.</p>
					</dd>

					<dt><a href="">References</a></dt>
					<dd>
						<p>Applebee, A. N. &amp; Langer, J. A. (2006). <em>The state of writing instruction in America&rsquo;s schools: What existing data   tell us</em>. Albany, NY: Center on English Learning and Achievement.</p>
						<p>Attali, Y. (2004). <em>Construct validity of e-rater in scoring TOEFL essays</em> (Research Report). Princeton, NJ: Educational Testing Service.</p>
						<p>Bennett, R. E. (2003). <em>Online assessment and the comparability of score meaning</em>. Princeton, NJ: Educational Testing Service.</p>
						<p>Bridgeman, B., Morgan, R., &amp; Wang, M. M. (1997). Choice among essay topics: Impact on performance and validity. <em>Journal of Educational Measurement, 34</em>(3): 273-286.</p>
						<p>Chung, G. K. W. K. &amp; O&rsquo;Neil, H. F. (1997). <em>Methodological approaches to online scoring of essays</em> (CSE Technical Report No. 461). Los Angeles: National Center for Research on Evaluation, Standards, and Student Testing.</p>
						<p>Crusan, D. (2002). An assessment of ESL writing placement assessment. <em>Assessing Writing, 8</em>, 17-30.</p>
						<p>Heck, R. H. &amp; Crislip, M. (2001). Direct and indirect writing assessments: Examining issues of equity and utility. <em>Educational Evaluation and Policy Analysis, 23</em>(1): 19-36.</p>
						<p>Huot, B. (1990). The literature of direct writing assessment: Major concerns and prevailing trends. <em>Review of Educational Research, 60</em>(2): 237-263.</p>
						<p>Lai, E. R., Wolfe, E. W., &amp; Vickers, D. H. (2012). <em>Halo effects and analytic scoring: A summary of two empirical studies</em> (Research Report). Iowa City, IA: Pearson.</p>
						<p>Landauer, T. K., Laham, D., &amp; Foltz, P. (2003). Automatic essay assessment. <em>Assessment in Education, 10</em>(3): 295-308.</p>
						<p>Linn, R. L., Betebenner, D. W., &amp; Wheeler, K. S. (1998). <em>Problem choice by test-takers: Implications for comparability and construct validity</em> (CSE Technical Report No. 495). Los Angeles: National Center for Research on Evaluation, Standards, and Student Testing.</p>
						<p>National Assessment Governing Board. (2010). <em>Writing framework for the 2011 National Assessment of Educational Progress</em>. Washington, D.C.</p>
						<p>Nichols, P. D. (2004, April). Evidence for the interpretation and use of scores from an automated essay scorer. Paper presented at the Annual Meeting of the American Educational Research Association (AERA), San Diego, CA.</p>
						<p>Page, E. B. (2003). Project essay grade: PEG. In M. D. Shermis &amp; J. Burstein (Eds.),<em>Automated essay scoring: A cross-disciplinary perspective</em> (pp. 43–54). Mahwah, NJ: Lawrence Erlbaum Associates.</p>
						<p>Quellmalz, E. S. (1984). Designing writing assessments: Balancing fairness, utility, and cost. <em>Educational Evaluation and Policy Analysis, 6</em>(1): 63-72.</p>
						<p>Quellmalz, E., Capell, F. J., &amp; Chou, C. P. (1980). <em>Defining writing: Effects of discourse and response mode</em> (CSE Report No. 132). Center for the Study of Evaluation: Los Angeles.</p>
						<p>Stiggins, R. J. (1982). A comparison of direct and indirect writing assessment methods.<em>Research in the Teaching of English, 16</em>(2): 101-114.</p>
						<p>Wainer, H. &amp; Thissen, D. (1994). On examinee choice in educational testing. <em>Review of Educational Research</em>, 64(1): 159-195.</p>
						<p>Willingham, W. W., &amp; Cole, N. S. (1997). <em>Gender and fair assessment</em>. Mahwah, NJ:  Erlbaum.</p>
					</dd>

				</dl>
				
				<hr class="subDivider"/>
				<article>
					<div class="pageButtons">
						<a class="pageButton" href="#_approach">Approach &raquo;</a><a class="pageButton" href="#_exemplars">Exemplars &raquo;</a>
					</div>
				</article>

			</section>
		</div>
	</body>
</html>
