<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<title>Online Games and Simulated Environments</title>
	</head>

	<body>
		<nav class="floatingMenu">
			<a href="#approach" class="selected">Approach</a>
			<a href="#exemplars">Exemplars</a>
			<a href="#moreinfo">More Info</a>
		</nav>
		<div class="approachPage">
			<section id="approach">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>Online Games and Simulated Environments</h1>
					</article>
				</div>
				<hr />
				<article>
					<div class='brainsharkWrapper'></div>
					<h2>Online Games and Simulated Environments</h2>
					<p>Online games and simulated environments are immersive, computer-based settings that feature digital interfaces with interactive, manipulable objects and animations.</p>
					<h5>In general, features of online games and simulated environments include:</h5>
					<ul>
						<li>Activities rather than discrete items</li>
						<li>Activities set in rich environments that include representations of multiple objects, circumstances, conditions, and interactions, providing context for the problem to be solved</li>
						<li>Opportunities to provide immediate, customized formative feedback that can be embedded in the experience</li>
						<li>The collection of richer information than is typically collected in traditional assessment contexts, including information about examinee process, work products, and goal attainment </li>
					</ul>
					
					<div class="pageButtons">
						<a class="pageButton" href="#_exemplars">Exemplars &raquo;</a><a class="pageButton" href="#_moreinfo">More Info &raquo;</a>
					</div>
					
				</article>
			</section>
			<section id="exemplars">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>Examples of Online Games and Simulated Environments</h1>
					</article>
				</div>
				<hr />
				<article>
					<h3>Below is a list of examples that demonstrate some of the ways that online games and simulated environments may be used as performance assessment.</h3>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://atlantisremixed.org/">Quest Atlantis: Taiga Park (Atlantis Remixed)</a></h3>
					<p>Atlantis Remixed (ARX) is a teaching and learning project that immerses children in educational tasks by using a 3D multi-user environment.</p>
					<p><em>Retrieved December 11, 2012 from</em> <a href="http://atlantisremixed.org/">http://atlantisremixed.org</a></p>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://rivercity.activeworlds.com/">River City (Harvard University and Arizona State University)</a></h3>
					<p>In River City, students use technology to research, form hypotheses, conduct experiments, and make recommendations to help solve the city's growing health problems.</p>
					<p><em>Retrieved June 10, 2013 from</em> <a href="http://rivercity.activeworlds.com/">http://rivercity.activeworlds.com/</a></p>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://www.dd.ucar.edu/">Hurricane Landfall (National Center for Atmospheric Research)</a></h3>
					<p>Hurricane Landfall is a strategy and negotiation computer game that teaches players about interactions between natural hazards and human decisions in a barrier island community.</p>
					<p><em>Retrieved December 11, 2012 from</em> <a href="http://www.dd.ucar.edu/">http://www.dd.ucar.edu/</a></p>
				</article>
				<hr class="subDivider" />
				<article>
					<h3><a href="http://archive.org/details/SavePatch">Save Patch (CRESST and USC&rsquo;s Game Innovation Lab)</a></h3>
					<p>This mathematics game for middle school students was developed to teach fraction concepts.</p>
					<p><em>Retrieved December 11, 2012 from</em> <a href="http://archive.org/details/SavePatch">http://archive.org/details/SavePatch</a></p>
				</article>

				<hr class="subDivider" />
				<article>
					<div class="pageButtons">
						<a class="pageButton" href="#_approach">Approach &raquo;</a><a class="pageButton" href="#_moreinfo">More Info &raquo;</a>
					</div>
				</article>

			</section>
			<section id="moreinfo">
				<div class="pageHeader">
					<article>
						<div class="chevrons"></div>
						<h1>What Are Online Games and Simulated Environments?</h1>
					</article>
				</div>
				<hr />
				<dl class="accordion">
					<dt><a href="">Definition</a></dt>
					<dd>
						<p>Online games and simulated environments are immersive, computer-based settings that feature digital interfaces with interactive, manipulable objects and animations. These environments often represent detailed fictional or real-world contexts. Educational games and simulations often blur the boundaries between learning and assessment by 1) supporting ubiquitous, unobtrusive, and ongoing assessment opportunities; 2) providing immediate and customized formative feedback; and 3) identifying valuable opportunities for additional learning. </p>
						<p>The technology underpinning games and simulated environments allows the collection of rich information about examinee processes, work products, and goal attainment. Such information can support inferences about complex types of knowledge, skills, and attributes, including 21st century competencies like critical thinking, collaboration, and persistence. Multiple learner attributes can be collected and scored to construct multidimensional profiles. </p>
						<p>Online games and simulated environments, when used as assessment approaches, are believed to stimulate student interest, engagement, and motivation compared to traditional learning and assessment approaches.</p>
					</dd>

					<dt><a href="">Characteristic Features</a></dt>
					<dd>
						<ul>
							<li>Feature digital interfaces with graphic and sometimes audio components and interactive, manipulable objects and animations</li>
							<li>Focus on activities rather than discrete items</li>
							<li>Typically, offer activities set in rich environments that include representations of multiple objects, circumstances, conditions, and interactions, providing context for the problem to be solved</li>
							<li>Can be embedded in daily learning activities rather than isolated as drop-in interruptions to the teaching-learning process</li>
							<li>Enable opportunities to provide immediate, customized formative feedback that can be embedded in the experience and provide natural consequences for actions</li>
							<li>May identify additional learning experiences through adaptive activity selection</li>
							<li>Are designed to collect richer information than is typically collected in traditional assessment contexts, including information about examinee processes, work products, and goal attainment</li>
						</ul>
						<p>As a result of these features, games and simulated environments: </p>
						<ul>
							<li>Are believed to increase student interest, engagement, and motivation</li>
							<li>Often blur the boundaries between learning and assessment</li>
							<li>Enable ubiquitous, unobtrusive, and ongoing assessment (Behrens et al., 2012); the continuous nature of assessment opportunities is what distinguishes this approach from technology-enhanced items, which are discrete and of finite duration</li>
							<li>Can support inferences about more complex types of knowledge, skills, and attributes, including critical thinking, problem solving or decision-making, planning, coordination or collaboration, persistence over time, and interest</li>
							<li>Allow development of multidimensional knowledge, skill, or attribute profiles based on a single performance</li>
						</ul>
						<p><em>The characteristic features identified are based on currently available research, literature, and examples. This approach to assessment is so new and the field is evolving so rapidly that these features will continue to change.</em></p>
					</dd>

					<dt><a href="">Common Contexts</a></dt>
					<dd>
						<p>These are used to simulate real-world phenomena, such as the relationship between fish reproduction and nutrification in a computer model of an aquatic ecosystem; virtual labs, such as virtual dissections of animals; and multiplayer virtual environments, such as online games that support collaborative problem solving.</p>
					</dd>

					<dt><a href="">Design Variations and Other Considerations</a></dt>
					<dd>
						<p>Games and simulated environments can vary in several different ways in response to design decisions that expand or constrain the problem space, tool space, solution space, and/or response space (Behrens et al., 2012). Problem space refers to the types of problems presented to users, whether open-ended or more constrained. Thus, the problem or goal a user must tackle may be more structured (e.g., carry out an experiment to determine the impact of nitric acid on different types of metal) or less structured (e.g., figure out why fish are dying in a local river).</p>
						<p>Similarly, the tools or affordances presented within the environment may be faithful to the types of tools a person would have access to in the &ldquo;real&rdquo; world (e.g., surgical tools in a simulation of an operating theatre) or may expand the types of tools to include those that are too expensive or simply not feasible to provide in live environments (e.g., real-time visual display of individual blood cells moving through the patient&rsquo;s heart).</p>
						<p>The solution space, which refers to the set of activities available to the user for solving the problem or achieving the goal, may also be expanded or constrained. For example, the solution space could be constrained to include only limited manipulation of the animation (e.g., varying the &ldquo;doses&rdquo; of the independent variable in an experiment) or could be expanded to include a range of possible activities (e.g., designing the experimental setup, interacting with informed agents, or choosing from among multiple settings to explore).</p>
						<p>The response space can also be expanded or constrained to produce variation in games and simulations. The response space refers to aspects of user performance within the game or simulation that will be evaluated or scored. For example, user responses may come in the form of selected responses to questions that exist outside the simulation (constrained), but may also include free responses to open-ended writing prompts (expanded). Also, as noted by Behrens et al. (2012), technology allows the consideration of both traditional work products (e.g., users&rsquo; written responses), as well as users&rsquo; interactions with the system (e.g., actions taken, objects manipulated, agents questioned).</p>
						<p>The degree of fidelity or authenticity of the simulation can also be varied, ranging from relatively simple and crude representations to finely textured and detailed representations. However, Behrens et al. (2012) warn against both &ldquo;over&rdquo; and &ldquo;under&rdquo; simulating aspects of the environment. Thus, simulations should represent material modifications to the problem, tool, solution, or response spaces. However, simulations should not bombard users with unnecessary details that place a burden on cognitive load and introduce construct-irrelevant variance.</p>
						<p>Within games and simulations that include a storyline or scenario, the complexity of these settings can vary from simple narratives with one or two characters to complicated settings involving multiple locales, detailed histories, and several characters.</p>
						<p>Games and simulations can vary depending on whether they are designed to allow or require collaboration or coordination with other players. Some games are set up as solitary experiences, whereas other games are designed to allow interactions among multiple users (as in multiplayer online games or multiuser virtual environments) or to require collaboration among players in order to evaluate how skillfully users interact with others.</p>
						<p>There are still relatively few examples of online games and simulated environments that have been explicitly designed as formal assessments of user knowledge, skills, and attributes. On the other hand, there appear to be a growing number of examples of online games and simulations developed for learning or entertainment purposes. Game developers and simulation researchers have so far focused on trying to use data that is routinely collected within these applications to support inferences about users&rsquo; knowledge, skills, and attributes that might inform future learning. However, if games and simulated environments are ever to be used to support high-stakes or summative inferences about what students know and can do, they will have to be designed more purposefully to enable such uses.</p>
					</dd>

					<dt><a href="">Response Demands</a></dt>
					<dd>
						<p>The response demands of online games and simulations vary according to their purposes and functionality. However, typically users are required to engage in some combination of the following: exploring the environment, identifying goals, inferring rules, manipulating variables, planning, monitoring progress toward goal completion, collaborating or coordinating one&rsquo;s actions with others, and solving problems or making decisions.</p>
					</dd>

					<dt><a href="">Evaluation Criteria and Procedures</a></dt>
					<dd>
						<p>Games and simulations provide opportunities to assess knowledge, skills, and attributes (KSAs) that have proven difficult to measure, collect, and score using traditional assessment approaches. Such KSAs include cognitive skills, such as critical or creative thinking; cognitive attributes, such as motivation and persistence; and affective attributes, such as interest or enjoyment. Often, multiple attributes can be evaluated or scored on the basis of a single performance, enabling the construction of multidimensional profiles.</p>
						<p>However, the ubiquity of data within digital environments can potentially result in a deluge of information. In this case, the challenge becomes parsing and aggregating the data in ways that can support meaningful inferences about users. For example, the methods for making inferences from log files that record user activity streams are still being developed.</p>
						<p>In addition, in order to provide feedback in real time and enable adaptive activity selection, automated scoring of user responses is required. This requirement can, at times, place limitations on the response space as assessment designers must stay within bounds of currently available scoring technology.</p>
						<p>Evaluation and scoring can become complicated in the case of multi-user games or simulated environments, particularly when users are either allowed or encouraged to collaborate with others. This is because assessment designers are typically interested in evaluating individual student KSAs, but opportunities for collaboration can obscure individual contributions (Webb, 1995). </p>
						<p>Research shows that group-level assessments may not yield scores that are predictive of individual-level ability, even when individual students produce separate work products. In particular, scores from group work tend to over-estimate individual performance and exhibit both ceiling effects and range restriction (Webb, 1993). If this higher performance reflects real student learning, then opportunities for collaboration do not necessarily invalidate multi-user experiences as measures of individual student outcomes. However, if students obtain higher scores from these collaborative experiences simply because their more able counterparts complete most of the work, then viewing scores from multi-user game play as indicators of individual student learning is problematic.</p>
					</dd>

					<dt><a href="">Administration Time</a></dt>
					<dd>
						<p>Games and simulations are distinguished from other technology-enhanced assessment approaches by their ongoing and continuous nature. Technology allows easy accumulation of data over longer periods of time (e.g., several months or years). Thus, games and simulations can be &ldquo;administered&rdquo; to a user on an &ldquo;anytime&rdquo; basis, allowing for measurement opportunities that are both distributed (as opposed to massed) and individualized (as opposed to on-demand).</p>
						<p>It should also be noted that games and simulations may not be the most efficient way to collect information about basic skills. It would take longer to play through a game to assess basic addition skills than to give a timed test of facts. The game may tell you more about the learner&rsquo;s ability to apply the skills in context, but if automaticity with facts is the construct of interest, a game or simulation may not be the best assessment approach.</p>
					</dd>

					<dt><a href="">References</a></dt>
					<dd>
						<p>Behrens, J. T., DiCerbo, K. E., &amp; Ferrara, &amp; S. (2012). Intended and unintended deceptions in the use of     simulations. Paper presented at the Invitational Research Symposium on Technology Enhanced Assessment,  Washington, DC.</p>
						<p>Chung, G. K. W. K. &amp; Baker, E. L. (1997). <em>Year 1 technology studies: implications for technology in assessment</em> (CSE Technical Report No. 459). Los Angeles, CA: University of California, National Center for Research on Evaluation, Standards, and Student Testing.</p>
						<p>Quellmalz, E. S., Timms, M.J., &amp; Schneider, S. A. (2009). <em>Assessment of student learning in science simulations and games</em>. Washington, DC: National Research Council.</p>
						<p>Rupp, A. A., Levy, R., DiCerbo, K. E., Sweet, S. J., Crawford, A. V., Calico, T., Benson, M., Fay, D., Kunze, K. L., Mislevy, R. J., Behrens, J. T. (2012). Putting ECD into practice: The interplay of theory and data in evidence models within a digital learning environment. <em>Journal of Educational Data Mining</em>, 4(1), 49-110.</p>
						<p>Rupp, A. A., Nugent, R., &amp; Nelson, B. (2012). Evidence-centered design for diagnostic assessment within digital learning environments: Integrating modern psychometrics and educational data mining. <em>Journal of Educational Data Mining</em>, 4(1), 1-10.</p>
						<p>Shute, V. J. (2011). Stealth assessment in computer-based games to support learning. In S. Tobias &amp; J. D. Fletcher (Eds.), <em>Computer games and instruction</em> (pp. 503-524). Charlotte, NC: Information Age Publishers. </p>
					</dd>

				</dl>
				
				<hr class="subDivider"/>
				<article>
					<div class="pageButtons">
						<a class="pageButton" href="#_approach">Approach &raquo;</a><a class="pageButton" href="#_exemplars">Exemplars &raquo;</a>
					</div>
				</article>

			</section>
		</div>
	</body>
</html>
